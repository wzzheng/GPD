<!doctype html>
<html>
<head>
<title>GPD-1</title>
<meta name="viewport" content="width=device-width,initial-scale=1">
<!-- <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous"> -->
<link href="assets/css/bootstrap.min.css" rel="stylesheet">
<!-- <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script> -->
<!-- <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet"> -->
<link href="assets/css/opensans.css" rel="stylesheet">
<link rel="icon" href="images/logo3.png">
<link href="assets/css/style.css" rel="stylesheet">
<style>
  .container_2{
    position: relative;
    width: 100%;
    height: 0;
    padding-bottom: 56.25%;
}
.video {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
}

  .collapsible {
    background-color: #777;
    color: white;
    cursor: pointer;
    padding: 18px;
    width: 100%;
    border: none;
    text-align: left;
    outline: none;
    font-size: 15px;
  }

  .active, .collapsible:hover {
    background-color: #555;
  }
  
  .content {
    padding: 0 18px;
    max-height: 0;
    overflow: hidden;
    transition: max-height 0.2s ease-out;
    background-color: #f1f1f1;
  }
</style>

<style>
.paperthumb {
  float:left; width: 120px; margin: 3px 10px 7px 0;
}
.paperdesc {
  clear: both;
}
</style>
</head>

<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <p class="lead" style="font-size:30px">
    <b><a href="https://arxiv.org/abs/2412.08643">GPD-1: Generative Pre-training for Driving</a></b>
  <address style="font-size: 110%;">
    <nobr><a href="https://github.com/rainyNighti">Zixun Xie</a><sup>*</sup>,</nobr>
    <nobr><a href="https://github.com/zuosc19">Sicheng Zuo</a><sup>*</sup>,</nobr>
    <nobr><a href="https://wzzheng.net/">Wenzhao Zheng</a><sup>*†</sup>,</nobr>
    <nobr><a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao">Yunpeng Zhang</a>,</nobr>
    <nobr><a href="TODO">Dalong Du</a>,</nobr>
    <nobr><a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en">Jie Zhou</a>,</nobr>
    <nobr><a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,</nobr>
    <nobr><a href="https://scholar.google.com/citations?user=voqw10cAAAAJ&hl=en">Shanghang Zhang</a><sup>‡</sup>
  <br>
      <nobr>Peking University</nobr>, 
      <nobr>Tsinghua University</nobr>,
      <nobr>PhiGent Robotics</nobr>
  </address>
   <!-- <div style="font-size: 170%;">CVPR 2023</div> -->
  <address style="font-size: 120%;">
	 <!-- <br> -->
  [<a href="https://arxiv.org/pdf/2412.08643"><b>Paper (Arxiv)</b></a>]&nbsp;&nbsp;&nbsp;&nbsp;
  <!-- [<a href="https://www.youtube.com/">Video(Youtube)</a>]&nbsp;&nbsp;&nbsp;&nbsp; -->
  [<a href="https://github.com/wzzheng/GPD"><b>Code (GitHub)</b></a>]&nbsp;&nbsp;&nbsp;&nbsp;
  [<a href="https://github.com/wzzheng/LDM"><b>Large Driving Models</b></a>]&nbsp;&nbsp;&nbsp;&nbsp;
  <!-- [<a href="https://zhuanlan.zhihu.com">Post(Zhihu)</a>] -->
  </address>
  <small>*Equal contribution. †Project Leader. ‡Corresponding author.</small>
 </div>
 </p>
 </div>
</div> <!-- end nd-pageheader -->

<div class="container">


<p align="center">
  <video width="90%" controls>
    <source src="assets/videos/SceneSimulation.mp4" type="video/mp4">
  </video>
</p>


<!-- <p><b>Overview of our contributions.</b> 
  Considering the universal approximating ability of Gaussian mixture, we propose an object-centric 3D semantic Gaussian representation to describe the fine-grained structure of 3D scenes without the use of dense grids. 
  We propose a GaussianFormer model consisting of sparse convolution and cross-attention to efficiently transform 2D images into 3D Gaussian representations. 
  To generate dense 3D occupancy, we design a Gaussian-to-voxel splatting module that can be efficiently implemented with CUDA. 
  With comparable performance, our GaussianFormer reduces memory consumption of existing 3D occupancy prediction methods by 75.2% - 82.2%.
</p> -->


<h2>Abstract</h2><hr>
<p>
  Modeling the evolutions of driving scenarios is important for the evaluation and decision-making of autonomous driving systems.
  Most existing methods focus on one aspect of scene evolution such as map generation, motion prediction, and trajectory planning.
  In this paper, we propose a unified Generative Pre-training for Driving (GPD) model to accomplish all these tasks altogether without additional finetuning.
  We represent each scene with ego, agent, and map tokens and formulate autonomous driving as a unified token generation problem.
  We adopt the autoregressive transformer architecture and use a scene-level attention mask to enable intra-scene bi-directional interactions.
  For the ego and agent tokens, we propose a hierarchical positional tokenizer to effectively encode both 2D positions and headings.
  For the map tokens, we train a map vector-quantized autoencoder to efficiently compress ego-centric semantic maps into discrete tokens.
  We pre-train our GPD on the large-scale nuPlan dataset and conduct extensive experiments to evaluate its effectiveness.
  With different prompts, our GPD successfully generalizes to various tasks without finetuning, including scene generation, traffic simulation, closed-loop simulation, map prediction, and motion planning.
</p>

<p align="center">
  <img src="assets/images/demo.png" width="90%">
</p>
<p>
  Given past 2D BEV observations, our pre-trained GPD model can jointly predict future scene evolution and agent movements. 
</p>


<h2>Overview of the GPD Approach</h2><hr>
<p align="center">
  <img src="assets/images/approach.png" width="90%">
</p>
<p>
  Our model adapts the GPT-like architecture for autonomous driving scenarios with two key innovations: 
  1) a 2D map scene tokenizer based on VQ-VAE that generates discrete, high-level representations of the 2D BEV map, and 2) a hierarchical quantization agent tokenizer to encode agent information. 
  Using a scene-level mask, the autoregressive transformer predicts future scenes by conditioning on both ground-truth and previously predicted scene tokens during training and inference, respectively.
</p>


<p align="center">
     <img src="assets/images/agentTokenizer.png" width="40%">
</p>
<p>
  Illustration of the hierarchical quantization agent tokenizer. 
  We designed a set of thresholds to categorize agent states into distinct ranges, converting continuous information into discrete representations.
</p>

<h2>Results</h2><hr>

<h4>Main Results</h4><hr>
<p align="center">
  <img src="assets/images/task_demonstration.png" width="40%">
</p>
<p>
  GPD can perform the four tasks: <strong>Scene Generation</strong>, <strong>Traffic Simulation</strong>, <strong>Closed-Loop Simulation</strong>, and <strong>Motion Planning</strong> without pre-training. 
  In our experiments, we provide a fixed 2-second map and agent data as initial information and use different prompt settings. The symbol "√" indicates that the selected ground-truth (GT) will be sent to the model as a prompt, while the remaining values will be generated by GPD. <br>
  Below, we present video demos for the other tasks separately, except for Scene Generation, which is featured at the top of the website.
</p>
<p align="center">
  <video width="90%" controls>
    <source src="assets/videos/TrafficSimulation.mp4" type="video/mp4">
  </video>
  <p>Traffic Simulation Demo</p>
</p>
<p align="center">
  <video width="90%" controls>
    <source src="assets/videos/CLS.mp4" type="video/mp4">
  </video>
  <p>Closed-Loop Simulation Demo</p>
</p>
<p align="center">
  <video width="90%" controls>
    <source src="assets/videos/MotionPlanning.mp4" type="video/mp4">
  </video>
  <p>Motion Planning Demo</p>
</p>
<p align="center">
  <img src="assets/images/mainres.png" width="80%">
  <p>The specific metrics for the four generation tasks.</p>
</p>
<p></p>


<h4>nuPlan Motion Planning Challenge</h4><hr>
<p align="center">
  <img src="assets/images/nuplanChallenge.png" width="90%">
  <p>
    We added only a single decoder layer to decode the ego token to meet the nuPlan challenge requirements. 
    Without relying on complex data augmentation or post-processing techniques, our model achieves performance comparable to PlanTF and even surpasses it in certain metrics.
  </p>
</p>


<h4>Map Prediction</h4><hr>
<p align="center">
  <video width="90%" controls>
    <source src="assets/videos/MapPredition.mp4" type="video/mp4">
  </video>
  <p>Map Prediction Demo</p>
</p>
<p align="center">
  <img src="assets/images/MapPrediction.png" width="40%">
</p>
<p>
  In the map generation experiment, we evaluated the model under two settings: 
  first, providing ground truth for both the agents and the ego vehicle to generate the map; 
  second, providing only the ego ground truth and making all other agents invisible to generate the map. 
</p>
<p></p>


<h4>Visualizations</h4><hr>
<p align="center">
  <img src="assets/images/V1.png" width="90%">
</p>
<P>
  Figure 4 shows the performance under the Scene Generation setting in complex scenarios. 
  The results demonstrate that even in highly intricate road conditions, the map can be generated smoothly. 
  In two turning scenarios, both the ego vehicle and agents follow a natural trajectory at a relatively steady speed. 
  Similarly, in two straight-driving scenarios, the model effectively captures surrounding agents' actions (e.g., turning, driving, and decelerating) while maintaining a stable forward speed.
</P>
<p align="center">
  <img src="assets/images/V2.png" width="90%">
</p>
<P>
  Figure 5 illustrates the performance in a more complex intersection-turning scenario across different settings. 
  The quality of map generation is notably satisfactory, and for both agents and the ego vehicle, the model’s performance closely matches the ground truth in all tasks, except where ground truth data is explicitly used. 
  This consistency highlights the robustness of our model.
</P>
<p></p>





<p>
<div class="card">
<h3 class="card-header">Bibtex</h3>
<div class="card-block">
<pre class="card-text clickselect">
  @article{gpd-1,
    title={GPD-1: Generative Pre-training for Driving},
    author={Xie, Zixun and Zuo, Sicheng and Zheng, Wenzhao and Zhang, Yunpeng and Du, Dalong and Zhou, Jie and Lu, Jiwen and Zhang, Shanghang},
    journal={arXiv preprint arXiv:2412.08643},
    year={2024}
}
</pre>
</div>
</div>
</p>



</div>
</div> <!-- row -->

</div> <!-- container -->

<script>
  var coll = document.getElementsByClassName("collapsible");
  var i;
  
  for (i = 0; i < coll.length; i++) {
    coll[i].addEventListener("click", function() {
      this.classList.toggle("active");
      var content = this.nextElementSibling;
      if (content.style.maxHeight){
        content.style.maxHeight = null;
      } else {
        content.style.maxHeight = content.scrollHeight * 50+ "px";
      } 
      content.style.height = "550%";
    });
  }
</script>

</body>
</html>


